# Projeto-AnÃ¡lise-de-dados-Pipeline-de-dados-telegram

### âš™ï¸ Arquitetura do Projeto:
<img src="architecture.png" alt="arquitetura">

### ğŸ“‹ Sobre o Projeto:
Nesse Projeto serÃ¡ construÃ­do um bot do Telegram com o objetivo de fazer um pipeline de dados com as conversas do Telegram, com intuito de simular um bot para ajudar o usuÃ¡rio com alguma dÃºvida, envolvendo etapas de ingestÃ£o de dados, ETL e apresentaÃ§Ã£o no ambiente da Amazon Web Service.


### ğŸ’» Ambientes utilizados:
<div>
 <img src="https://img.shields.io/badge/Amazon_AWS-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white" alt="aws Logo">
 <img src="https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252" alt="icon colab">
</div>

### âŒ¨ï¸ Linguagens utilizadas:
<div>
  <img src="https://cdn-icons-png.flaticon.com/128/5968/5968350.png" alt="Python Logo"  width="40px">
  <img src="https://cdn-icons-png.flaticon.com/128/9544/9544010.png" alt="sql Logo"  width="40px">
</div>


### âœ… Desenvolvimento:

* CriaÃ§Ã£o de um bot de Telegram com o uso do botfather e configuraÃ§Ã£o do bot para atender as necessidades do projeto.
* Retorno de funÃ§Ãµes Python em resposta a interaÃ§Ã£o com o bot do Telegram.
* Captura das mensagens do bot e demais dados com o mÃ©todo `get`.
* Trabalho com trÃªs etapas no pipeline:
   * IngestÃ£o
   * ETL
   * ApresentaÃ§Ã£o
* Na etapa de IngestÃ£o envolve coleta, transferÃªncia e armazenamento de dados.
   - Criar um *bucket* no `AWS S3`;
   - Criar uma funÃ§Ã£o no `AWS Lambda`;
        - Definir variÃ¡veis de ambiente e permissÃµes.
   - Criar uma API *web* no `AWS API Gateway`;
   - Configurar o *webhook* da API de *bots* do **Telegram**.
* Na etapa de ETL envolve extraÃ§Ã£o e transformaÃ§Ã£o dos dados para o usuÃ¡rio final.
   - Criar um *bucket* no `AWS S3` com sufixo `-enriched`;
   - Criar uma funÃ§Ã£o no `AWS Lambda` com sufixo `-enriched`;
        - Definir variÃ¡veis de ambiente, permissÃµes, recursos e camadas.
   - criar regra para ativar a funÃ§Ãµe de ETL do `AWS Lambda` no `AWS Event Bridge`;
        - Definir programaÃ§Ã£o para execuÃ§Ã£o da funÃ§Ã£o.
* Por fim, na etapa de ApresentaÃ§Ã£o usamos o `AWS Athena` para apresentar para o usuÃ¡rio final informaÃ§Ãµes do bot por meio de consultas `SQL`.
   -  o `AWS Athena` tem funÃ§Ã£o de entregar o dados atravÃ©s de uma interface `SQL` para os usuÃ¡rios do sistema analÃ­tico;
   -  Com o dado disponÃ­vel, usuÃ¡rio podem executar as mais variadas consultas analÃ­ticas;

  
### ğŸ“¥ ImportaÃ§Ãµes Python:

â® **Google Colab**:

```
import os
import pandas as pd
import logging
import json
import requests
from getpass import getpass
import seaborn as sns

import pyarrow as pa
import pyarrow.parquet as pq

```
â® **AWS Lambda**:

```
import os
import json
import logging
from datetime import datetime, timezone
import boto3
from datetime import datetime, timedelta, timezone
import pyarrow as pa
import pyarrow.parquet as pq

```
### ğŸ“ OrganizaÃ§Ã£o do Projeto:
------------


    â”œâ”€â”€ requirements.txt          <- O arquivo de requisitos para reproduzir o ambiente de anÃ¡lise, por exemplo, gerado com `pip congelamento > requisitos.txt
    â”‚
    â”œâ”€â”€ LICENSE
    â”‚
    â”œâ”€â”€ README.md                 <- O Readme de nÃ­vel superior para desenvolvedores que usam esse projeto
    â”‚
    â”œâ”€â”€ architecture.png          <- Arquitetura do projeto de Pipeline de dados   
    â”‚
    â”œâ”€â”€ Query                     <- Referente as consultas SQL realizadas.
    â”‚
    â””â”€â”€notebook                  <- caderno jupyter notebook utilizado para contruÃ§Ã£o e apresentaÃ§Ã£o do projeto
    

--------
### ğŸŒ ReferÃªncias:

âœ¦ [ebac](https://ebaconline.com.br/)  âœ¦ [hashtag_programacao]((https://github.com/andre-marcos-perez/data-pipeline-demo))

