# Projeto-An√°lise-de-dados-Pipeline-de-dados-telegram

### ‚öôÔ∏è Arquitetura do Projeto:
<img src="architecture.png" alt="arquitetura">

### üìã Sobre o Projeto:
Nesse Projeto ser√° constru√≠do um bot do Telegram com o objetivo de fazer um pipeline de dados com as conversas do Telegram, com intuito de simular um bot para ajudar o usu√°rio com alguma d√∫vida, envolvendo etapas de ingest√£o de dados, ETL e apresenta√ß√£o no ambiente da Amazon Web Service.


### üíª Ambientes utilizados:
<div>
 <img src="https://img.shields.io/badge/Amazon_AWS-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white" alt="aws Logo">
 <img src="https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252" alt="icon colab">
</div>

### ‚å®Ô∏è Linguagens utilizadas:
<div>
  <img src="https://cdn-icons-png.flaticon.com/128/5968/5968350.png" alt="Python Logo"  width="40px">
  <img src="https://cdn-icons-png.flaticon.com/128/9544/9544010.png" alt="sql Logo"  width="40px">
</div>


### ‚úÖ Desenvolvimento:

* Cria√ß√£o de um bot de Telegram com o uso do botfather e configura√ß√£o do bot para atender as necessidades do projeto.
* Retorno de fun√ß√µes Python em resposta a intera√ß√£o com o bot do Telegram.
* Captura das mensagens do bot e demais dados com o m√©todo `get`.
* Trabalho com tr√™s etapas no pipeline:
   * Ingest√£o
   * ETL
   * Apresenta√ß√£o
* Na etapa de Ingest√£o envolve coleta, transfer√™ncia e armazenamento de dados.
   - Criar um *bucket* no `AWS S3`;
   - Criar uma fun√ß√£o no `AWS Lambda`;
        - Definir vari√°veis de ambiente e permiss√µes.
   - Criar uma API *web* no `AWS API Gateway`;
   - Configurar o *webhook* da API de *bots* do **Telegram**.
* Na etapa de ETL envolve extra√ß√£o e transforma√ß√£o dos dados para o usu√°rio final.
   - Criar um *bucket* no `AWS S3` com sufixo `-enriched`;
   - Criar uma fun√ß√£o no `AWS Lambda` com sufixo `-enriched`;
        - Definir vari√°veis de ambiente, permiss√µes, recursos e camadas.
   - criar regra para ativar a fun√ß√µe de ETL do `AWS Lambda` no `AWS Event Bridge`;
        - Definir programa√ß√£o para execu√ß√£o da fun√ß√£o.
* Por fim, na etapa de Apresenta√ß√£o usamos o `AWS Athena` para apresentar para o usu√°rio final informa√ß√µes do bot por meio de consultas `SQL`.
   -  o `AWS Athena` tem fun√ß√£o de entregar o dados atrav√©s de uma interface `SQL` para os usu√°rios do sistema anal√≠tico;
   -  Com o dado dispon√≠vel, usu√°rio podem executar as mais variadas consultas anal√≠ticas;

  
### üì• Importa√ß√µes Python:

‚ûÆ **Google Colab**:

```
import os
import pandas as pd
import logging
import json
import requests
from getpass import getpass
import seaborn as sns

import pyarrow as pa
import pyarrow.parquet as pq

```
‚ûÆ **AWS Lambda**:

```
import os
import json
import logging
from datetime import datetime, timezone
import boto3
from datetime import datetime, timedelta, timezone
import pyarrow as pa
import pyarrow.parquet as pq

```
### üìé Organiza√ß√£o do Projeto:
------------


    ‚îú‚îÄ‚îÄ requirements.txt                        <- O arquivo de requisitos para reproduzir o ambiente de an√°lise.
    ‚îÇ
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îÇ
    ‚îú‚îÄ‚îÄ README.md                               <- Resumo do projeto como um todo.
    ‚îÇ
    ‚îú‚îÄ‚îÄ architecture.png                        <- Arquitetura do projeto de Pipeline de dados.  
    ‚îÇ
    ‚îú‚îÄ‚îÄ Query                                   <- Referente as consultas SQL realizadas.
    ‚îÇ
    ‚îî‚îÄ‚îÄProject_Pipeline_Telegram_Lucas.ipynb    <- caderno jupyter notebook utilizado para contru√ß√£o e apresenta√ß√£o do projeto.
    

--------
### üåê Refer√™ncias:

‚ú¶ [ebac](https://ebaconline.com.br/)  ‚ú¶ [hashtag_programacao]((https://github.com/andre-marcos-perez/data-pipeline-demo))

